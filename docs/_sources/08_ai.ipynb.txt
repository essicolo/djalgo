{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Machine learning\n",
    "\n",
    "> **Note**. Djalgo's AI approach is experimental. \n",
    "\n",
    "We introduced machine learning while fitting Gaussian processes in section [5. Walks](05_walks.html). Djalgo's module `djai` includes tools for modeling music from MIDI data relying on Tensorflow (a package for deep learning). `djai` is not loaded by default when importing Djalgo, since otherwise Tensorflow, a large and complicated package, should have been added to Djalgo's dependencies. To use `djalgo`, you must [install Tensorflow](https://www.tensorflow.org/install) in your environment. `djai` also rely on Pretty-midi to load and process MIDI files: you should also install it with `!pip install pretty-midi`. `djai` should be loaded as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 15:07:15.500285: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import djalgo as dj\n",
    "from djalgo import djai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `djalgo` module is a sophisticated Python library designed for processing MIDI files using deep learning models. It extracts pitch, durations, offsets, an extra timing feature (time delta) and the (one-hot encoded) track the note belongs to. This module is could be useful for music researchers, AI enthusiasts, and developers working in the domain of automated music generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before goind into coding..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics: art as the witnesses of experience\n",
    "\n",
    "My ethos will fluctuate and evolve, as anything should in the precious, short time we exist. Their is nothing inherently wrong woth AI, but if your piece was generating with a banal command prompt, your creative process is anything but banal and uninteresting, no matter the result. In times when any artistic piece needed years of work, the result was more important than the process. Now, when anyone can ask a LLM to generate an image of a cat riding a dinausar in a 5D space in the style of a mixed of Daly and cyber-punk, well, results are generated within seconds, and the process becomes more relevant. If, like me, you have spent years to designed your own AI, the *process* (not the result) behind the musical piece has an artistic value as good as any composer who has spent those years studying musical theory. Artists are people who spent the precious time they own to think on the narration of the object they created. When the process becomes applying reciepe, it belongs to home sweet home printed carpets sold on Amazon.\n",
    "\n",
    "The `djai` module doesn't come with pre-trained models. That would have been too easy, right? I prefer seeing you tweak it and train it with your own compositions rather than just use it on Leonard Cohen song to generate new one. You worth more than this, and the world deserves more than command-prompt artists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features\n",
    "\n",
    "`djai` has the following features.\n",
    "\n",
    "1. **MIDI File Scanning**: Scans directories for MIDI files, allowing for selective processing based on user-defined limits.\n",
    "1. **Feature Extraction**: Extracts musical features such as pitch, duration, and timing from MIDI files.\n",
    "1. **Data Preprocessing**: Handles scaling and one-hot encoding of musical features for neural network processing.\n",
    "1. **Model Training and Prediction**: Supports building and training of LSTM and Transformer-based models for music prediction.\n",
    "1. **Music Generation**: Generates new music tracks by predicting sequences of musical notes.\n",
    "\n",
    "## Components\n",
    "\n",
    "There are three classes in `djai`. The `DataProcessor` class is used internally tomanages feature extraction and sequence generation from MIDI files and performs preprocessing tasks such as feature scaling and encoding. `DataProcessor` is automatically called in the second class, `ModelManager`, which facilitates the creation, training, and management of neural network models. `ModelManager` supports three kinds of architectures: *LSTM*, *GRU* and *transformer* and provides functionalities for model training, prediction, and music generation. The third class, `PositionalEncoding`, is a custom Tensorflow layer used internally to build transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The maestro data set comprises hundreds of midi files. Only three were selected to showcase the `DjFlow` class. To scan the files, use the `scan_midi_files` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "midi_files = djai.scan_midi_files('_djai-files/_maestro-sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be created with a class instanciation comprising a long list of arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "deep_djmodel = djai.ModelManager(\n",
    "    sequence_length_i=30, sequence_length_o=10,\n",
    "    num_instruments=1, model_type='gru',\n",
    "    n_layers=5, n_units=64, dropout=0.1, batch_size=32,\n",
    "    learning_rate=0.005, loss_weights=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Model Configuration in `djalgo`\n",
    "\n",
    "#### Key Parameters and Their Impact on Model Performance\n",
    "\n",
    "In the `djalgo` module, several parameters play critical roles in defining how the neural network learns and generates music based on MIDI files. Let's break down these parameters for better clarity.\n",
    "\n",
    "##### Sequence Length\n",
    "- **`sequence_length_i`** and **`sequence_length_o`** determine the number of notes the model uses to make predictions. Specifically, `sequence_length_i` refers to the number of input notes used to predict the next `sequence_length_o` notes. For example, setting `sequence_length_i` to 30 and `sequence_length_o` to 10 means the model uses 30 notes to predict the subsequent 10 notes.\n",
    "\n",
    "##### Number of Instruments\n",
    "- **`num_instruments`** specifies how many different instruments the model should consider. This parameter is crucial for models trained on diverse ensembles. Note that training on MIDI files with fewer instruments than specified can lead to inefficiencies and unnecessary computational overhead.\n",
    "\n",
    "##### Model Type\n",
    "- **`model_type`** can be set to `'lstm'`, `'gru'`, or `'transformer'`:\n",
    "  - **LSTMs** (Long Short-Term Memory networks) are more traditional and capable but tend to be complex.\n",
    "  - **GRUs** (Gated Recurrent Units) aim to simplify the architecture of LSTMs with fewer parameters while maintaining performance.\n",
    "  - **Transformers** are at the forefront of current large language model (LLM) technology, offering potentially superior learning capabilities due to their attention mechanisms, albeit at the cost of increased complexity and computational demands.\n",
    "\n",
    "##### Architecture Configuration\n",
    "- **`n_layers`** and **`n_units`** control the depth and width of the neural network. `n_layers` is the number of layers in the network, and `n_units` represents the number of neurons in each of these layers.\n",
    "\n",
    "##### Training Dynamics\n",
    "- **`dropout`** is a technique to prevent overfitting by randomly deactivating a portion of the neurons during training, specified by a ratio between 0 and 1.\n",
    "- **`batch_size`** affects how many samples are processed before the model updates its internal parameters, impacting both training speed and convergence behavior.\n",
    "- **`learning_rate`** influences the step size at each iteration in the training process. A higher learning rate can cause overshooting optimal solutions, while a very low rate may lead to slow convergence.\n",
    "\n",
    "##### Loss Weights\n",
    "- **`loss_weights`** allows customization of the importance of different prediction components such as pitch, duration, offset, and time delta, potentially skewing the model to prioritize accuracy in specific areas.\n",
    "\n",
    "### Fitting the Model\n",
    "\n",
    "To train the model, you use the `.fit()` method with a list of MIDI file paths. The number of epochs, which represent complete passes over the entire dataset, can be adjusted according to the complexity of the task and desired accuracy. More epochs typically lead to better model performance but require more time to complete.\n",
    "\n",
    "This configuration gives a comprehensive view of how `djalgo` harnesses advanced neural network architectures to generate music, allowing users to tailor the learning process to specific needs and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 103ms/step - instrument_index_accuracy: 0.9812 - loss: 7.2406\n",
      "Epoch 2/5\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 105ms/step - instrument_index_accuracy: 0.9994 - loss: 2.6191\n",
      "Epoch 3/5\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 98ms/step - instrument_index_accuracy: 1.0000 - loss: 2.5502\n",
      "Epoch 4/5\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 99ms/step - instrument_index_accuracy: 1.0000 - loss: 2.3235\n",
      "Epoch 5/5\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 98ms/step - instrument_index_accuracy: 1.0000 - loss: 2.1008\n"
     ]
    }
   ],
   "source": [
    "history = deep_djmodel.fit(midi_files, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models are long to fit, so you might want to save it for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "deep_djmodel.save('_djai-files/lstm.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict a new sequence, you can use the `.generate()` method of the ModelManager object. The predict method takes the first notes of a MIDI file (defined in `sequence_length_i`) and returns a Djalgo track or, for multiple instruments, a list of tracks. Make sure that the MIDI file has enough notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(71, 0.0021451586, 1.2516425),\n",
       "  (71, 0.0021453972, 1.2528331),\n",
       "  (71, 0.0021450035, 1.2544166),\n",
       "  (71, 0.0021549403, 1.2575257),\n",
       "  (71, 0.00215076, 1.2582741),\n",
       "  (71, 0.0021510506, 1.2592133),\n",
       "  (71, 0.0021390854, 1.2571211),\n",
       "  (71, 0.002152808, 1.2582511),\n",
       "  (71, 0.002152864, 1.2610896),\n",
       "  (71, 0.0021475654, 1.2620306)]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = deep_djmodel.generate(midi_files[0], length=10)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "Predictions are clearly not suited yet for music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
