{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 09:59:20.708889: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import music21 as m21\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "\n",
    "def scan_midi_files(directory, max_files=None):\n",
    "    \"\"\"\n",
    "    Scans the specified directory for MIDI files using glob with a while loop.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The directory to scan for MIDI files.\n",
    "        max_files (int, optional): The maximum number of files to scan. If None, all files are scanned.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of MIDI files found.\n",
    "    \"\"\"\n",
    "    search_pattern = os.path.join(directory, '**', '*.mid*')\n",
    "    midi_files = []\n",
    "\n",
    "    # Utiliser glob.iglob pour obtenir un itérateur\n",
    "    for file in glob.iglob(search_pattern, recursive=True):\n",
    "        midi_files.append(file)\n",
    "        if max_files is not None and len(midi_files) >= max_files:\n",
    "            break\n",
    "\n",
    "    return midi_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import music21 as m21\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, sequence_length_i, sequence_length_o, num_instruments, scale_mins=None, scale_maxs=None):\n",
    "        self.sequence_length_i = sequence_length_i\n",
    "        self.sequence_length_o = sequence_length_o\n",
    "        self.total_sequence_length = sequence_length_i + sequence_length_o\n",
    "        self.num_instruments = num_instruments\n",
    "        self.numerical_indices = slice(0, 3) # hard coded, pitch, duration, tick_delta\n",
    "        self.instrument_encoding = {}\n",
    "        self.scale_mins = scale_mins\n",
    "        self.scale_maxs = scale_maxs\n",
    "\n",
    "    def encode_instruments(self, part):\n",
    "        if part.partName not in self.instrument_encoding:\n",
    "            if len(self.instrument_encoding) < self.num_instruments:\n",
    "                self.instrument_encoding[part.partName] = len(self.instrument_encoding)\n",
    "            else:\n",
    "                return np.zeros(self.num_instruments)  # Return an empty one-hot vector\n",
    "        index = self.instrument_encoding.get(part.partName, 0)\n",
    "        one_hot = np.zeros(self.num_instruments)\n",
    "        one_hot[index] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def extract_features(self, notes, instrument_vector):\n",
    "        pitches = []\n",
    "        durations = []\n",
    "        tick_deltas = []\n",
    "        \n",
    "        # Ensure that notes are sorted correctly by their offsets\n",
    "        notes.sort(key=lambda x: x.offset)\n",
    "\n",
    "        # Use the first note's offset as the starting point for delta calculations\n",
    "        previous_offset = notes[0].offset\n",
    "\n",
    "        for i, note in enumerate(notes):\n",
    "            current_offset = note.offset  # Handle both chords and notes uniformly\n",
    "            if note.isChord:\n",
    "                # If it's a chord, process each note in the chord\n",
    "                for chord_note in note.notes:\n",
    "                    if len(pitches) < self.total_sequence_length:\n",
    "                        pitches.append(chord_note.pitch.midi)\n",
    "                        durations.append(chord_note.duration.quarterLength)\n",
    "                        tick_deltas.append(current_offset - previous_offset)  # Delta calculation\n",
    "                        previous_offset = current_offset  # Update previous offset after processing the chord\n",
    "            elif note.isNote:\n",
    "                if len(pitches) < self.total_sequence_length:\n",
    "                    pitches.append(note.pitch.midi)\n",
    "                    durations.append(note.duration.quarterLength)\n",
    "                    tick_deltas.append(current_offset - previous_offset)\n",
    "                    previous_offset = current_offset\n",
    "\n",
    "        # Adjust the first tick delta to zero for the sequence start\n",
    "        if tick_deltas:\n",
    "            tick_deltas[0] = 0\n",
    "\n",
    "        # Combine features and tile the instrument vector\n",
    "        features = np.column_stack((pitches, durations, tick_deltas))\n",
    "        instrument_features = np.tile(instrument_vector, (len(pitches), 1))\n",
    "        return np.column_stack((features, instrument_features))\n",
    "\n",
    "\n",
    "    def midi_files_to_sequences(self, midi_files):\n",
    "        all_sequences = []\n",
    "        for midi_file in midi_files:\n",
    "            score = m21.converter.parse(midi_file)\n",
    "            for part in score.parts:\n",
    "                instrument_vector = self.encode_instruments(part)\n",
    "                if instrument_vector is None:\n",
    "                    continue\n",
    "\n",
    "                notes = list(part.flatten().notesAndRests)\n",
    "                notes.sort(key=lambda note: note.offset)\n",
    "                # Iterate through the notes to extract all possible sequences of the defined length\n",
    "                for i in range(len(notes) - self.total_sequence_length + 1):\n",
    "                    sequence = notes[i:i + self.total_sequence_length]\n",
    "                    features = self.extract_features(sequence, instrument_vector)\n",
    "                    if features.shape[0] == self.total_sequence_length:  # Ensure only complete sequences are added\n",
    "                        all_sequences.append(features)\n",
    "\n",
    "        return np.array(all_sequences, dtype=object)\n",
    "\n",
    "    def compute_scaling_parameters(self, sequences):\n",
    "        self.scale_mins = np.nanmin(sequences[:, :, self.numerical_indices], axis=(0, 1))\n",
    "        self.scale_maxs = np.nanmax(sequences[:, :, self.numerical_indices], axis=(0, 1))\n",
    "        self.scale_mins[0] = 0  # Min pitch\n",
    "        self.scale_maxs[0] = 127  # Max pitch\n",
    "\n",
    "    def scale_numerical_features(self, sequences):\n",
    "        if self.scale_mins is None or self.scale_maxs is None:\n",
    "            self.compute_scaling_parameters(sequences)\n",
    "        sequences[:, :, self.numerical_indices] = (sequences[:, :, self.numerical_indices] - self.scale_mins) / (self.scale_maxs - self.scale_mins)\n",
    "        return sequences\n",
    "\n",
    "    def prepare_data(self, midi_files):\n",
    "        self.sequences = self.midi_files_to_sequences(midi_files)\n",
    "        if len(self.sequences) == 0:\n",
    "            raise ValueError(\"No sequences were extracted. Check the input data, maybe just a wrong path definition.\")\n",
    "        if self.num_instruments == 1:\n",
    "            self.sequences = self.sequences[:, :, self.numerical_indices] # remove the instrument feature if only one instrument is scanned\n",
    "        self.sequences_sc = self.scale_numerical_features(self.sequences.copy())\n",
    "        sequences_input = np.array(self.sequences_sc[:, :self.sequence_length_i, :], dtype=np.float32)\n",
    "        sequences_output = self.sequences_sc[:, self.sequence_length_i:, :]\n",
    "        sequences_output_list = [np.array(sequences_output[:, :, i], dtype=np.float32) for i in range(self.numerical_indices.stop)]\n",
    "        sequences_output_list.append(\n",
    "            np.array(sequences_output[:, :, self.numerical_indices.stop:], dtype=np.float32)\n",
    "        )\n",
    "        return sequences_input, tuple(sequences_output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model, **kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.position = position\n",
    "        self.d_model = d_model\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
    "                                     np.arange(d_model)[np.newaxis, :],\n",
    "                                     d_model)\n",
    "        sines = np.sin(angle_rads[:, 0::2])\n",
    "        cosines = np.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = np.concatenate([sines, cosines], axis=-1)[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'position': self.position,\n",
    "            'd_model': self.d_model\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, sequence_length_i=30, sequence_length_o=10, num_instruments=2, model_type='lstm', n_layers=3, n_units=128, dropout=0.2, batch_size=32, learning_rate=0.005, num_heads=2, loss_weights=None):\n",
    "        \"\"\"\n",
    "        sequence_length_o is not 1, even though we work in autoregression. Predicting multiple steps ahead even though subsequent steps are ignored is called teacher forcing.\n",
    "        \"\"\"\n",
    "\n",
    "        self.sequence_length_i = sequence_length_i\n",
    "        self.sequence_length_o = sequence_length_o\n",
    "        self.num_instruments = num_instruments\n",
    "        self.num_numeric_features = 3  # pitch, duration, tick_delta\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        if loss_weights is None:\n",
    "            # Set default loss weights\n",
    "            self.loss_weights = {'pitch': 1.0, 'duration': 1.0, 'tick_delta': 1.0}\n",
    "            if num_instruments > 1:\n",
    "                self.loss_weights['instrument_index'] = 1.0  # Add instrument loss weight \n",
    "        else:\n",
    "            self.loss_weights = loss_weights\n",
    "        self.model = self._create_default_model(n_layers, n_units, dropout, num_heads, model_type, num_instruments)\n",
    "\n",
    "        self.data_processor = DataProcessor(sequence_length_i, sequence_length_o, num_instruments)\n",
    "        self.model = self._create_default_model(n_layers, n_units, dropout, num_heads, model_type, num_instruments)\n",
    "\n",
    "    def _create_default_model(self, n_layers, n_units, dropout, num_heads, model_type, num_instruments):\n",
    "        if num_instruments < 2:\n",
    "            n_features_onehot = self.num_numeric_features\n",
    "        else:\n",
    "            n_features_onehot = self.num_numeric_features + num_instruments\n",
    "        input_shape = (self.sequence_length_i, n_features_onehot)\n",
    "        inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "        x = inputs\n",
    "        if model_type == 'transformer':\n",
    "            positional_encoding_layer = PositionalEncoding(self.sequence_length_i, n_features_onehot)\n",
    "            x = positional_encoding_layer(x)\n",
    "            for i in range(n_layers):\n",
    "                attention_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=n_units)(x, x)\n",
    "                x = tf.keras.layers.Dropout(dropout)(x)\n",
    "                x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
    "                ff_output = tf.keras.Sequential([\n",
    "                    tf.keras.layers.Dense(n_units, activation='relu'),\n",
    "                    tf.keras.layers.Dense(x.shape[-1]),\n",
    "                ])(x)\n",
    "                x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + ff_output)\n",
    "        else:\n",
    "            for i in range(n_layers):\n",
    "                if model_type == 'lstm':\n",
    "                    x = tf.keras.layers.LSTM(n_units, return_sequences=True, dropout=dropout)(x)\n",
    "                elif model_type == 'gru':\n",
    "                    x = tf.keras.layers.GRU(n_units, return_sequences=True, dropout=dropout)(x)\n",
    "\n",
    "        x = x[:, -self.sequence_length_o:]\n",
    "\n",
    "        outputs = [\n",
    "            tf.keras.layers.Dense(1, name=f\"{feature}\")(x) for feature in [\"pitch\", \"duration\", \"tick_delta\"]\n",
    "        ]\n",
    "        if num_instruments > 1:\n",
    "            instruments_output = tf.keras.layers.TimeDistributed(\n",
    "                tf.keras.layers.Dense(num_instruments, activation='softmax'), name='instrument_index'\n",
    "            )(x)\n",
    "            outputs.append(instruments_output)\n",
    "\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            self.learning_rate,\n",
    "            decay_steps=10000,\n",
    "            decay_rate=0.96,\n",
    "            staircase=True\n",
    "        )\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        loss_dict = {'pitch': 'mean_squared_error',\n",
    "                    'duration': 'mean_squared_error',\n",
    "                    'tick_delta': 'mean_squared_error'}\n",
    "        if num_instruments > 1:\n",
    "            loss_dict['instrument_index'] = 'categorical_crossentropy'\n",
    "        \n",
    "        model.compile(optimizer=optimizer,\n",
    "                    loss=loss_dict,\n",
    "                    loss_weights=self.loss_weights,\n",
    "                    metrics={'instrument_index': 'accuracy'} if num_instruments > 1 else None)\n",
    "        return model\n",
    "    \n",
    "    def fit(self, midi_files, epochs=10):\n",
    "        inputs, targets = self.data_processor.prepare_data(midi_files)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).batch(self.batch_size)\n",
    "        history = self.model.fit(dataset, epochs=epochs)\n",
    "        return history\n",
    "        \n",
    "    def scale_back(self, predicted_outputs):\n",
    "        unscaled_outputs = []\n",
    "        for i, p in enumerate(predicted_outputs):\n",
    "            if i < self.data_processor.numerical_indices.stop:  # Handle numerical features\n",
    "                min_val = self.data_processor.scale_mins[i]\n",
    "                max_val = self.data_processor.scale_maxs[i]\n",
    "                unscaled_outputs.append(p * (max_val - min_val) + min_val)\n",
    "            else:  # Handle one-hot encoded instruments\n",
    "                exp_logits = np.exp(p)\n",
    "                softmax_output = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "                unscaled_outputs.append(softmax_output)\n",
    "        return unscaled_outputs\n",
    "\n",
    "    def generate(self, midi_file_path, length=10):\n",
    "        if isinstance(midi_file_path, str):\n",
    "            midi_file_path = [midi_file_path]\n",
    "        inputs, _ = self.data_processor.prepare_data(midi_file_path)\n",
    "        if inputs.size == 0:\n",
    "            print(\"No sequences extracted, possibly too few notes.\")\n",
    "            return None\n",
    "\n",
    "        current_sequences = inputs[0:1, :self.sequence_length_i, :]  # Initial input\n",
    "        generated_sequences = [[] for _ in range(self.num_instruments)]\n",
    "\n",
    "        for _ in range(length):\n",
    "            predictions = self.model.predict(current_sequences, verbose=0)\n",
    "            scaled_predictions = self.scale_back(predictions)\n",
    "\n",
    "            for i in range(self.num_instruments):\n",
    "                pitch = int(round(scaled_predictions[0][0, -1, 0]))  # Assumes pitch is the first feature\n",
    "                duration = scaled_predictions[1][0, -1, 0]  # Assumes duration is the second feature\n",
    "                offset = scaled_predictions[2][0, -1, 0]  # Assumes offset is the third feature\n",
    "                if self.num_instruments > 1:\n",
    "                    instrument_probabilities = scaled_predictions[3][0, -1, :]  # Instrument classification\n",
    "                    chosen_instrument = np.argmax(instrument_probabilities)\n",
    "                    if chosen_instrument == i:\n",
    "                        generated_sequences[i].append((pitch, duration, offset))\n",
    "                else:\n",
    "                    generated_sequences[i].append((pitch, duration, offset))\n",
    "\n",
    "                # Update the sequence for the next prediction\n",
    "                next_step_features = []\n",
    "                for i in range(len(predictions)):\n",
    "                    if i < self.data_processor.numerical_indices.stop:\n",
    "                        next_step_features.append(predictions[i][0, 0, 0])\n",
    "                    else:\n",
    "                        next_step_features.extend([p for p in predictions[i][0, 0, :]])\n",
    "                next_step_features = np.array(next_step_features, dtype=np.float32).reshape(1, 1, -1)\n",
    "                current_sequences = np.concatenate([current_sequences[:, 1:, :], next_step_features], axis=1)\n",
    "        \n",
    "        if len(generated_sequences) == 1:\n",
    "            return generated_sequences[0]\n",
    "        else:\n",
    "            return generated_sequences\n",
    "\n",
    "    def save(self, filepath):\n",
    "        self.model.save(filepath)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filepath):\n",
    "        return tf.keras.models.load_model(filepath, custom_objects={'PositionalEncoding': PositionalEncoding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager = ModelManager(\n",
    "    sequence_length_i=32, sequence_length_o=8,\n",
    "    num_instruments=1, model_type='lstm',\n",
    "    n_layers=8, n_units=256, dropout=0.25, batch_size=32,\n",
    "    learning_rate=0.001, num_heads=4,\n",
    "    loss_weights={'pitch': 1.0, 'duration': 1.0, 'tick_delta': 1.0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_files = scan_midi_files('_midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 602ms/step - loss: 0.0707\n",
      "Epoch 2/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 556ms/step - loss: 0.0187\n",
      "Epoch 3/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 564ms/step - loss: 0.0157\n",
      "Epoch 4/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 600ms/step - loss: 0.0113\n",
      "Epoch 5/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 612ms/step - loss: 0.0111\n",
      "Epoch 6/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 807ms/step - loss: 0.0109\n",
      "Epoch 7/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 573ms/step - loss: 0.0109\n",
      "Epoch 8/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 547ms/step - loss: 0.0108\n",
      "Epoch 9/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 541ms/step - loss: 0.0110\n",
      "Epoch 10/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 535ms/step - loss: 0.0107\n"
     ]
    }
   ],
   "source": [
    "history = model_manager.fit(midi_files, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_midi_file = '../../../midi/pl_score.mid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_manager.generate(pred_midi_file, length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(62, 1.0771412, 0.2630389),\n",
       " (62, 1.0771034, 0.26300684),\n",
       " (62, 1.0770981, 0.2630125),\n",
       " (62, 1.0770462, 0.26296777),\n",
       " (62, 1.0770552, 0.26299468),\n",
       " (62, 1.0770465, 0.26300126),\n",
       " (62, 1.0770648, 0.2629898),\n",
       " (62, 1.0770886, 0.26303563),\n",
       " (62, 1.077089, 0.2630591),\n",
       " (62, 1.0771168, 0.2631155),\n",
       " (62, 1.0771198, 0.2631478),\n",
       " (62, 1.0771376, 0.26319808),\n",
       " (62, 1.0771053, 0.26319587),\n",
       " (62, 1.0771608, 0.26329595),\n",
       " (62, 1.0771418, 0.26337194),\n",
       " (62, 1.0771418, 0.26336756),\n",
       " (62, 1.077198, 0.26346183),\n",
       " (62, 1.0771985, 0.26348665),\n",
       " (62, 1.0772007, 0.2635134),\n",
       " (62, 1.077186, 0.2635199),\n",
       " (62, 1.0772426, 0.26355383),\n",
       " (62, 1.0772609, 0.26358485),\n",
       " (62, 1.0773149, 0.26362547),\n",
       " (62, 1.0773038, 0.26361158),\n",
       " (62, 1.0773242, 0.26363987),\n",
       " (62, 1.0772942, 0.26360962),\n",
       " (62, 1.0773116, 0.26363274),\n",
       " (62, 1.0773578, 0.26368597),\n",
       " (62, 1.0773737, 0.263663),\n",
       " (62, 1.0773535, 0.26363036),\n",
       " (62, 1.0773637, 0.26364046),\n",
       " (62, 1.0773802, 0.26365665),\n",
       " (62, 1.0773511, 0.26361904),\n",
       " (62, 1.0773511, 0.26361912),\n",
       " (62, 1.0773511, 0.26361895),\n",
       " (62, 1.0773511, 0.2636193),\n",
       " (62, 1.0773511, 0.2636192),\n",
       " (62, 1.0773511, 0.2636193),\n",
       " (62, 1.077351, 0.26361948),\n",
       " (62, 1.0773511, 0.26361963),\n",
       " (62, 1.0773509, 0.26361948),\n",
       " (62, 1.0773509, 0.26361963),\n",
       " (62, 1.0773509, 0.2636193),\n",
       " (62, 1.0773509, 0.2636193),\n",
       " (62, 1.0773511, 0.26361963),\n",
       " (62, 1.0773511, 0.2636193),\n",
       " (62, 1.0773511, 0.26361963),\n",
       " (62, 1.0773511, 0.2636199),\n",
       " (62, 1.0773511, 0.26361963),\n",
       " (62, 1.0773511, 0.26361954),\n",
       " (62, 1.0773511, 0.2636193),\n",
       " (62, 1.0773511, 0.26361948),\n",
       " (62, 1.0773511, 0.26361948),\n",
       " (62, 1.0773511, 0.26361963),\n",
       " (62, 1.0773513, 0.26361948),\n",
       " (62, 1.0773511, 0.26362),\n",
       " (62, 1.0773513, 0.26361972),\n",
       " (62, 1.0773511, 0.26361972),\n",
       " (62, 1.0773511, 0.2636199),\n",
       " (62, 1.0773511, 0.26362),\n",
       " (62, 1.0773511, 0.26361948),\n",
       " (62, 1.0773511, 0.26361972),\n",
       " (62, 1.0773511, 0.26361963),\n",
       " (62, 1.0773511, 0.26361948),\n",
       " (62, 1.0773513, 0.26362008),\n",
       " (62, 1.0773511, 0.2636198),\n",
       " (62, 1.0773511, 0.26361963),\n",
       " (62, 1.0773513, 0.2636198),\n",
       " (62, 1.0773511, 0.26361954),\n",
       " (62, 1.0773513, 0.26361948),\n",
       " (62, 1.0773511, 0.26362),\n",
       " (62, 1.0773511, 0.26361954),\n",
       " (62, 1.0773511, 0.2636198),\n",
       " (62, 1.0773511, 0.26361954),\n",
       " (62, 1.0773513, 0.26361963),\n",
       " (62, 1.0773511, 0.2636193),\n",
       " (62, 1.0773511, 0.2636198),\n",
       " (62, 1.0773511, 0.26361963),\n",
       " (62, 1.0773511, 0.26361954),\n",
       " (62, 1.0773513, 0.26361954),\n",
       " (62, 1.0773513, 0.2636198),\n",
       " (62, 1.0773511, 0.26361963),\n",
       " (62, 1.0773511, 0.26361963),\n",
       " (62, 1.0773511, 0.26361948),\n",
       " (62, 1.0773513, 0.26361948),\n",
       " (62, 1.0773513, 0.26361972),\n",
       " (62, 1.0773511, 0.2636198),\n",
       " (62, 1.0773511, 0.2636198),\n",
       " (62, 1.0773513, 0.26361948),\n",
       " (62, 1.0773511, 0.26361963),\n",
       " (62, 1.0773511, 0.26361954),\n",
       " (62, 1.0773513, 0.26361963),\n",
       " (62, 1.0773511, 0.26361972),\n",
       " (62, 1.0773511, 0.26361948),\n",
       " (62, 1.0773511, 0.2636199),\n",
       " (62, 1.0773513, 0.2636192),\n",
       " (62, 1.0773513, 0.26362),\n",
       " (62, 1.0773511, 0.26361954),\n",
       " (62, 1.0773511, 0.26361948),\n",
       " (62, 1.0773513, 0.26361954)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
