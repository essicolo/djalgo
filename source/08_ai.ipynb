{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. ü§ñ Machine learning\n",
    "\n",
    "> **Note**. Djalgo's AI approach produces uniform outcomes. Want to help? github.com/essicolo/djalgo \n",
    "\n",
    "We introduced machine learning while fitting Gaussian processes in section [5. Walks](05_walks.html). Djalgo's module `djai` includes tools for modeling music from MIDI data relying on Tensorflow (a package for deep learning). `djai` is not loaded by default when importing Djalgo, since otherwise Tensorflow, a large and complicated package, should have been added to Djalgo's dependencies. To use `djai`, you must [install Tensorflow](https://www.tensorflow.org/install) in your environment. `djai` also rely on Music21 to load and process MIDI files: you should also install it with `!pip install music21`. Although Music21 is not as fast as Pretty-midi to process MIDI files, I had a better experience with processing files with it. `djai` should be loaded as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 11:52:38.056833: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import djalgo as dj\n",
    "from djalgo import djai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `djai` module is designed for processing MIDI files using deep learning models. It extracts pitch, durations, an offset computed in terms of difference in quarter lengths from the previous note (called tick delta) and the (one-hot encoded) track the note belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí≠ Ethics: art as the witnesses of experience\n",
    "\n",
    "Even though `djai` was the module which took me the most time to develop, it is these days, to my opinion, the least interesting. Who needs to DIY their own AI when interesting results can already be generated with a command prompt to an AI? My ethos will fluctuate and evolve, as anything should in the precious, short time we exist. Their is nothing inherently wrong woth AI, but if your piece was generating with a banal command prompt, your creative process is anything but banal and uninteresting, no matter the result. In times when any artistic piece needed years of work, the result was as, or more important than the process. Now, when anyone can ask a LLM to generate an image of a cat riding a dinausar in space in the style of a mixed of Daly and cyber-punk, well, results are generated within seconds, and the process becomes more relevant. Nonetheless, if, like me, you have spent months to designed your own AI, the *process* (not the result) behind the musical piece has an artistic value as good as any composer who has spent those months studying musical theory. Finally, artists are people who spent the precious time they own to think on the narration of the object they created. When the process becomes applying reciepe, it belongs to home sweet home printed carpets sold on Amazon.\n",
    "\n",
    "The `djai` module doesn't come with pre-trained models. That would have been too easy, right? I prefer seeing you tweak it and train it with your own compositions rather than just use it on Leonard Cohen song to generate new one. You worth more than this, and the world deserves more than command-prompt artists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóùÔ∏è Features\n",
    "\n",
    "`djai` has the following features.\n",
    "\n",
    "1. **MIDI File Scanning**: Scans directories for MIDI files, allowing for selective processing based on user-defined limits.\n",
    "1. **Feature Extraction**: Extracts musical features such as pitch, duration, and timing from MIDI files.\n",
    "1. **Data Preprocessing**: Handles scaling and one-hot encoding of musical features for neural network processing.\n",
    "1. **Model Training and Prediction**: Supports building and training models for music prediction.\n",
    "1. **Music Generation**: Generates new music tracks by predicting sequences of musical notes.\n",
    "\n",
    "## üß© Components\n",
    "\n",
    "There are three classes in `djai`. The `DataProcessor` class is used internally tomanages feature extraction and sequence generation from MIDI files and performs preprocessing tasks such as feature scaling and encoding. `DataProcessor` is automatically called in the second class, `ModelManager`, which facilitates the creation, training, and management of neural network models. `ModelManager` supports three kinds of architectures: *LSTM*, *GRU* and *transformer* and provides functionalities for model training, prediction, and music generation. The third class, `PositionalEncoding`, is a custom Tensorflow layer used internally to build transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü™ú Example\n",
    "\n",
    "I downloaded three midi files were selected to showcase DjAI. To scan the files, use the `scan_midi_files` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_midi/tetris.mid',\n",
       " '_midi/pinkpanther.mid',\n",
       " '_midi/adams.mid',\n",
       " '_midi/rocky.mid',\n",
       " '_midi/mario.mid']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_files = djai.scan_midi_files('_midi')\n",
    "midi_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be created with a class instanciation comprising a list of arguments, which are explained right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_manager = djai.ModelManager(\n",
    "    sequence_length_i=24, sequence_length_o=8,\n",
    "    n_instruments=1, model_type='gru',\n",
    "    n_layers=4, n_units=128, dropout=0.3, batch_size=64,\n",
    "    learning_rate=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéõÔ∏è Understanding Model Configuration in `djai`\n",
    "\n",
    "#### Key Parameters and Their Impact on Model Performance\n",
    "\n",
    "In the `djai` module, several parameters play critical roles in defining how the neural network learns and generates music based on MIDI files. Let's break down these parameters for better clarity.\n",
    "\n",
    "##### Sequence Length\n",
    "- `sequence_length_i` and `sequence_length_o` determine the number of notes the model uses to make predictions. Specifically, `sequence_length_i` refers to the number of input notes used to predict the next `sequence_length_o` notes. For example, setting `sequence_length_i` to 30 and `sequence_length_o` to 10 means the model uses 30 notes to predict the subsequent 10 notes. Even though the model predicts a sequence, DjAI retains only the first prediction, an approach named *teacher forcing*. The autoregressive approach removes the first item of the sequence to predict from, then append the newly predicted one as basis to predict the next. \n",
    "\n",
    "##### Number of Instruments\n",
    "- `n_instruments` specifies how many different instruments the model should consider, starting from the first of each MIDI file. This parameter is crucial for models trained on diverse ensembles. Note that training on MIDI files with fewer instruments than specified can lead to inefficiencies and unnecessary computational overhead.\n",
    "\n",
    "##### Model Type\n",
    "- `model_type` can be set to `'lstm'`, `'gru'`, or `'transformer'`:\n",
    "  - LSTMs (Long Short-Term Memory networks) are more traditional and capable but tend to be complex.\n",
    "  - GRUs (Gated Recurrent Units) aim to simplify the architecture of LSTMs with fewer parameters while maintaining performance.\n",
    "  - Transformers are at the forefront of current large language model (LLM) technology, offering potentially superior learning capabilities due to their attention mechanisms, albeit at the cost of increased complexity and computational demands.\n",
    "\n",
    "##### Architecture Configuration\n",
    "- `n_layers` and `n_units` control the depth and width of the neural network. `n_layers` is the number of layers in the network, and `n_units` represents the number of neurons in each of these layers. `n_heads` is the number of heads in the multi-head attention algorithm, and is only taken into account in the *transformer* model type.\n",
    "\n",
    "##### Training Dynamics\n",
    "- `dropout` is a technique to prevent overfitting by randomly deactivating a portion of the neurons during training, specified by a ratio between 0 and 1.\n",
    "- `batch_size` affects how many samples are processed before the model updates its internal parameters, impacting both training speed and convergence behavior.\n",
    "- `learning_rate` influences the step size at each iteration in the training process. A higher learning rate can cause overshooting optimal solutions, while a very low rate may lead to slow convergence.\n",
    "\n",
    "##### Loss Weights\n",
    "- `loss_weights` allows customization of the importance of different prediction components such as pitch, duration, offset, and time delta, potentially skewing the model to prioritize accuracy in specific areas.\n",
    "\n",
    "### üèãÔ∏è Fitting the Model\n",
    "\n",
    "To train the model, you use the `.fit()` method with a list of MIDI file paths. The number of epochs, which represent complete passes over the entire dataset, can be adjusted according to the complexity of the task and desired accuracy. More epochs typically lead to better model performance but require more time to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 71ms/step - loss: 0.1528\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0665\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 0.0585\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 0.0552\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 0.0537\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 0.0516\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 0.0523\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 0.0508\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.0503\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - loss: 0.0506\n"
     ]
    }
   ],
   "source": [
    "history = model_manager.fit(midi_files, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models are long to fit, so you might want to save it for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_manager.save('_output/lstm.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict a new sequence, you can use the `.generate()` method of the `djai.ModelManager` object. The generate method takes the first notes of a MIDI file (defined in `sequence_length_i`) and returns a Djalgo track or, for multiple instruments, a list of tracks. Make sure that the MIDI file has enough notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(68, 0.9924258, 0.28069848),\n",
       " (68, 0.99277496, 0.28115964),\n",
       " (68, 0.9936436, 0.28131378),\n",
       " (69, 0.99493337, 0.28136227),\n",
       " (69, 0.9964782, 0.28146863),\n",
       " (69, 0.9980702, 0.28165814),\n",
       " (69, 0.9995358, 0.28189823),\n",
       " (69, 1.0007924, 0.28215533),\n",
       " (69, 1.0018008, 0.28239486),\n",
       " (69, 1.0025604, 0.28259254)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model_manager.generate('_output/polyloop.mid', length=10)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "Predictions are uniform and clearly not suited yet for music yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicbook",
   "language": "python",
   "name": "musicbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
